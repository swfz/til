{
  "results": [
    {
      "hits": [
        {
          "date": "March 25, 2022",
          "title": "BigQueryの日付を扱う際のメモ",
          "slug": "/entries/bigquery_date_function/",
          "text": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day, # 月初\nLAST_DAY(CURRENT_DATE(), MONTH) AS last_day, # 月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS first_day_of_yesterday, # 前日起算の月初\nLAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS last_day_of_yesterday, # 前日起算の月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH), MONTH) AS first_day_of_last_three_month # 3ヶ月前の月初\n```\n\nDATE_TRUNC, LAST_DAYが便利",
          "timeToRead": 1,
          "objectID": "68f46908-591f-5bb4-82bd-f2fc099406d2",
          "_snippetResult": {
            "text": {
              "value": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day",
              "matchLevel": "none"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "March 25, 2022",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yの日付を扱う際のメモ",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_date_function/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day, # 月初\nLAST_DAY(CURRENT_DATE(), MONTH) AS last_day, # 月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS first_day_of_yesterday, # 前日起算の月初\nLAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS last_day_of_yesterday, # 前日起算の月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH), MONTH) AS first_day_of_last_three_month # 3ヶ月前の月初\n```\n\nDATE_TRUNC, LAST_DAYが便利",
              "matchLevel": "none",
              "matchedWords": []
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "May 08, 2021",
          "title": "BigQueryのbq load時にautodetectを使えない場合",
          "slug": "/entries/bigquery_cant_use_autodetect/",
          "text": "\nPocketのデータをAPIで取得してBigQueryに突っ込もうとしたときの話\n\nGCSにJSONを置いてCLIから`bq load --autodetect`でデータをloadしようとしたらエラーで怒られた\n\n```\nBigQuery error in load operation: Error processing job\n'project-111111:bqjob_r70118be7bda78ce4_000001793f9c2946_1': Error while reading\ndata, error message: JSON table encountered too many errors, giving up. Rows: 1;\nerrors: 1. Please look into the errors[] collection for more details.\nFailure details:\n- Error while reading data, error message: JSON processing\nencountered too many errors, giving up. Rows: 1; errors: 1; max\nbad: 0; error percent: 0\n- gs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json: Error\nwhile reading data, error message: JSON parsing error in row\nstarting at position 0: JSON object specified for non-record field:\nlist.videos\n```\n\n## 前提\n\n現状あるデータは次の3つ（bucket名はサンプル）\n\n```\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-03.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-05.json\n```\n\n## 原因の切り分け\n\n- raw-03.json\n- raw-04.json\n\nのときは問題なくloadできている\n\n`raw-05.json`\n\nが追加されてから上記エラーになってしまった\n\n05と03,04のJSONの中身を比べてみたところ05には`list.videos`がすべて`[]`になっていた\n\n03,04に関してはどこかのレコードでオブジェクトが入っていたので`RECORD`と判断された模様\n\nこのことから`--autodetect`は最初のファイルをautodetectで読み込んで順番にその他ファイルも読み込んでいると考えられる\n\n[スキーマの自動検出](https://cloud.google.com/bigquery/docs/schema-detect?hl=ja#auto-detect)\n\nここに説明が書いてあった\n\n> 自動検出を有効にすると、BigQuery はデータソース内でランダムにファイルを選択します。ファイルの最大 100 行をスキャンして代表的なサンプルとして使用し、推定プロセスを開始します。BigQuery は、各フィールドを検証し、そのサンプル内の値に基づいてそのフィールドにデータ型を割り当てようとします。\n\nランダムでファイルを読み込むとあるので全パターンを網羅したデータがあるファイルじゃないファイルがサンプルに選定されてしまった場合にこういうことが起きる\n\nそうなると`autodetect`は使えないのでテーブル作成→loadの手順を踏む必要がある\n\n- schemaの取り出し\n\nうまく行ったパターンで生成したテーブルのスキーマを取得する\n\n```\nbq show --schema --format=prettyjson pocket.rawdata > pocket-rawdata.json\n```\n\n- テーブル作成\n\n別のテーブルを用意して試してみる\n\n```\nbq mk --table --time_partitioning_field month --time_partitioning_type MONTH sample.pocket_rawdata pocket-rawdata.json\n```\n\n- load\n\n```\nbq load --source_format=NEWLINE_DELIMITED_JSON --replace sample.pocket_rawdata 'gs://sample-bucket/preprocessed_rawdata/*'\n```\n\n今のところこんな感じでなんとかなっている\n\n## まとめ\n\n取り込み対象のデータにばらつきがある（あるデータではRECORD、あるデータでは`[]`のようなとき）とサンプリング次第で取り込めない場合がある\n\nさらに`--autodetect`+`--replace`を用いると毎回ロード時に自動検出するので失敗する可能性がある\n\nそのためスキーマを定義してテーブルの作成+`bq load`と手順を踏む必要がある\n\n## 所感\n\n本来だったら`autodetect`で生成したスキーマからさらに精査して本当に`NULLABLE`?みたいな話も考えたほうが良いが今回は趣味プロジェクトなので…\n\nautodetectは便利だけどこういうパターンに対応できないのでやはりPOCやお試しのときくらいしか使えないよなーとあらためて感じた\n\nまぁでも気軽に試せるのはとても良いことなので使い分けが大事",
          "timeToRead": 3,
          "objectID": "511a0a9b-6cec-55d0-a965-148667fcf789",
          "_snippetResult": {
            "text": {
              "value": "\nPocketのデータをAPIで取得して__ais-highlight__BigQuer__/ais-highlight__yに突っ込もうとしたときの話\n\nGCSにJSON",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "May 08, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yのbq load時にautodetectを使えない場合",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_cant_use_autodetect/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nPocketのデータをAPIで取得して__ais-highlight__BigQuer__/ais-highlight__yに突っ込もうとしたときの話\n\nGCSにJSONを置いてCLIから`bq load --autodetect`でデータをloadしようとしたらエラーで怒られた\n\n```\n__ais-highlight__BigQuer__/ais-highlight__y error in load operation: Error processing job\n'project-111111:bqjob_r70118be7bda78ce4_000001793f9c2946_1': Error while reading\ndata, error message: JSON table encountered too many errors, giving up. Rows: 1;\nerrors: 1. Please look into the errors[] collection for more details.\nFailure details:\n- Error while reading data, error message: JSON processing\nencountered too many errors, giving up. Rows: 1; errors: 1; max\nbad: 0; error percent: 0\n- gs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json: Error\nwhile reading data, error message: JSON parsing error in row\nstarting at position 0: JSON object specified for non-record field:\nlist.videos\n```\n\n## 前提\n\n現状あるデータは次の3つ（bucket名はサンプル）\n\n```\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-03.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-05.json\n```\n\n## 原因の切り分け\n\n- raw-03.json\n- raw-04.json\n\nのときは問題なくloadできている\n\n`raw-05.json`\n\nが追加されてから上記エラーになってしまった\n\n05と03,04のJSONの中身を比べてみたところ05には`list.videos`がすべて`[]`になっていた\n\n03,04に関してはどこかのレコードでオブジェクトが入っていたので`RECORD`と判断された模様\n\nこのことから`--autodetect`は最初のファイルをautodetectで読み込んで順番にその他ファイルも読み込んでいると考えられる\n\n[スキーマの自動検出](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/schema-detect?hl=ja#auto-detect)\n\nここに説明が書いてあった\n\n> 自動検出を有効にすると、__ais-highlight__BigQuer__/ais-highlight__y はデータソース内でランダムにファイルを選択します。ファイルの最大 100 行をスキャンして代表的なサンプルとして使用し、推定プロセスを開始します。__ais-highlight__BigQuer__/ais-highlight__y は、各フィールドを検証し、そのサンプル内の値に基づいてそのフィールドにデータ型を割り当てようとします。\n\nランダムでファイルを読み込むとあるので全パターンを網羅したデータがあるファイルじゃないファイルがサンプルに選定されてしまった場合にこういうことが起きる\n\nそうなると`autodetect`は使えないのでテーブル作成→loadの手順を踏む必要がある\n\n- schemaの取り出し\n\nうまく行ったパターンで生成したテーブルのスキーマを取得する\n\n```\nbq show --schema --format=prettyjson pocket.rawdata > pocket-rawdata.json\n```\n\n- テーブル作成\n\n別のテーブルを用意して試してみる\n\n```\nbq mk --table --time_partitioning_field month --time_partitioning_type MONTH sample.pocket_rawdata pocket-rawdata.json\n```\n\n- load\n\n```\nbq load --source_format=NEWLINE_DELIMITED_JSON --replace sample.pocket_rawdata 'gs://sample-bucket/preprocessed_rawdata/*'\n```\n\n今のところこんな感じでなんとかなっている\n\n## まとめ\n\n取り込み対象のデータにばらつきがある（あるデータではRECORD、あるデータでは`[]`のようなとき）とサンプリング次第で取り込めない場合がある\n\nさらに`--autodetect`+`--replace`を用いると毎回ロード時に自動検出するので失敗する可能性がある\n\nそのためスキーマを定義してテーブルの作成+`bq load`と手順を踏む必要がある\n\n## 所感\n\n本来だったら`autodetect`で生成したスキーマからさらに精査して本当に`NULLABLE`?みたいな話も考えたほうが良いが今回は趣味プロジェクトなので…\n\nautodetectは便利だけどこういうパターンに対応できないのでやはりPOCやお試しのときくらいしか使えないよなーとあらためて感じた\n\nまぁでも気軽に試せるのはとても良いことなので使い分けが大事",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "3",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "December 09, 2020",
          "title": "BigQueryでサンプルデータをサクッと作る",
          "slug": "/entries/bigquery_sample_data/",
          "text": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ込まなくてもサンプルデータを生成できる\n\n```sql\nWITH sample AS(\n  SELECT * FROM UNNEST(ARRAY<STRUCT<start_date DATE, end_date DATE, item STRING, sales INT64>>\n    [\n      (\"2020-08-01\", \"2020-11-30\", \"hoge\", 100),\n      (\"2020-10-01\", \"2020-10-31\", \"fuga\", 200)\n    ]\n  )\n)\nSELECT * FROM sample\n```\n\n- 結果\n\n|start_date|end_date|item|sales|\n|---|---|---|---|\n|2020-08-01|2020-11-30|hoge|100|\n|2020-10-01|2020-10-31|fuga|200|\n\n\n記事書くときや説明とかに使える\n",
          "timeToRead": 1,
          "objectID": "28192504-51b0-5f94-9f12-c62f278c23cc",
          "_snippetResult": {
            "text": {
              "value": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ",
              "matchLevel": "none"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "December 09, 2020",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yでサンプルデータをサクッと作る",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_sample_data/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ込まなくてもサンプルデータを生成できる\n\n```sql\nWITH sample AS(\n  SELECT * FROM UNNEST(ARRAY<STRUCT<start_date DATE, end_date DATE, item STRING, sales INT64>>\n    [\n      (\"2020-08-01\", \"2020-11-30\", \"hoge\", 100),\n      (\"2020-10-01\", \"2020-10-31\", \"fuga\", 200)\n    ]\n  )\n)\nSELECT * FROM sample\n```\n\n- 結果\n\n|start_date|end_date|item|sales|\n|---|---|---|---|\n|2020-08-01|2020-11-30|hoge|100|\n|2020-10-01|2020-10-31|fuga|200|\n\n\n記事書くときや説明とかに使える\n",
              "matchLevel": "none",
              "matchedWords": []
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "April 21, 2021",
          "title": "BigQueryで日付を扱うときはTimezoneを意識する",
          "slug": "/entries/bigquery_date_timezone/",
          "text": "\ndataformでsourceテーブルから中間テーブルを生成してassertionを書いていた\n\n検算したら件数が合わないなーということで調べた\n\n次のようなSQLで`from`,`to`を指定して単月分のレコードのみ抜き出すというパターン\n\n```sql\nSELECT\n  *,\n  'private' AS workspace\nFROM\n  `sample.rawdata-private`,\n  UNNEST(data) AS d\nWHERE\n  DATE(start) BETWEEN ${target_date.from}\n  AND ${target_date.to}\n```\n\n<!-- textlint-disable prh -->\nSQLXなので`target_date.to`と`target_date.from`はその時々によって変化する\n<!-- textlint-enable prh -->\n\n今回は`2021-04-01` ～ `2021-04-30`をという感じ\n\n`rawdata-private`はAPIのレスポンスをそのまま保存していて1行に`total_count`と`data`列に実際のレコードがあるので`UNNEST`してレコード数と比較することで確認している\n\n`rawdata-private`のレコードを追ってみると\n\n```json\n\"start\": \"2021-04-01T04:57:39+09:00\",\n```\n\nのデータが`DATE(start)`を通すことで`2021-03-31`になっていた\n\nなるほどUTC\n\n`DATE(start, 'Asia/Tokyo'),`でタイムゾーン指定の日付データに変換できるのでこれで対応\n\nBigQueryがDATEでよしなにやってくれた結果UTCで解釈すると`2021-03-31`となってしまうためフィルタ対象から外れてしまい、件数が合わない状態になっていた\n\n正直assertion書いてなかったら気付かなかったのでassertion大事w",
          "timeToRead": 1,
          "objectID": "1d531f2a-2c16-5859-96ce-a8cf37a230b8",
          "_snippetResult": {
            "text": {
              "value": "ータに変換できるのでこれで対応\n\n__ais-highlight__BigQuer__/ais-highlight__yがDATEでよしなにやってくれた結",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "April 21, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yで日付を扱うときはTimezoneを意識する",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_date_timezone/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\ndataformでsourceテーブルから中間テーブルを生成してassertionを書いていた\n\n検算したら件数が合わないなーということで調べた\n\n次のようなSQLで`from`,`to`を指定して単月分のレコードのみ抜き出すというパターン\n\n```sql\nSELECT\n  *,\n  'private' AS workspace\nFROM\n  `sample.rawdata-private`,\n  UNNEST(data) AS d\nWHERE\n  DATE(start) BETWEEN ${target_date.from}\n  AND ${target_date.to}\n```\n\n<!-- textlint-disable prh -->\nSQLXなので`target_date.to`と`target_date.from`はその時々によって変化する\n<!-- textlint-enable prh -->\n\n今回は`2021-04-01` ～ `2021-04-30`をという感じ\n\n`rawdata-private`はAPIのレスポンスをそのまま保存していて1行に`total_count`と`data`列に実際のレコードがあるので`UNNEST`してレコード数と比較することで確認している\n\n`rawdata-private`のレコードを追ってみると\n\n```json\n\"start\": \"2021-04-01T04:57:39+09:00\",\n```\n\nのデータが`DATE(start)`を通すことで`2021-03-31`になっていた\n\nなるほどUTC\n\n`DATE(start, 'Asia/Tokyo'),`でタイムゾーン指定の日付データに変換できるのでこれで対応\n\n__ais-highlight__BigQuer__/ais-highlight__yがDATEでよしなにやってくれた結果UTCで解釈すると`2021-03-31`となってしまうためフィルタ対象から外れてしまい、件数が合わない状態になっていた\n\n正直assertion書いてなかったら気付かなかったのでassertion大事w",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "February 28, 2022",
          "title": "JSONファイルをBigQueryに読ませJSON型で扱うためにそのままCSVで保存する",
          "slug": "/entries/json_to_csv/",
          "text": "\n[Working with JSON data in Standard SQL  |  BigQuery  |  Google Cloud](https://cloud.google.com/bigquery/docs/reference/standard-sql/json-data)\n\n先日BigQueryでnative JSON型がプレビューでサポートされた\n\n執筆時点ではloadはCSVしか対応していないようだったのでJSONのファイルはCSVに変換する必要がある\n\nとりあえず使ってみるためにJSONを返すAPIのレスポンスをまるまるCSVにして突っ込んでみることにした\n\n```\ncat hoge.json| jq -r '[.|tostring]|@csv' > hoge.csv\n```\n\n- schema.json\n\n```json\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"data\",\n    \"type\": \"JSON\"\n  }\n]\n```\n\n### load\n\n```\nbq load --replace --source_format=CSV ${GOOGLE_PROJECT}:sample.content_text hoge.csv ./schema.json\n```\n\nこれでJSON型を使えるようになった\n\n1ファイル1レコードという力技だが扱う容量が多くなければこの方法でもいける\n\n何度かSQLたたいてみたけどSTRUCTやREPEATEDなど今まで型でサポートしてくれていた部分を考慮してあげないといけない\n\nなのでいったん特定のカラムを抜き出す工程みたいなのが必要\n\n配列も`JSON_QUERY_ARRAY`を挟んであげる必要があるなどまぁそうだよなという感じ\n\nload対象がJSONファイルで特定のキー以下をJSON型として扱うということができるようになってほしいと感じた\n\n現状だと上記のようにひと手間かけないといけないのでデータレイク的なところから一次整形処理を挟む必要が出てくるのでまだちょっと使いづらいなーという感じ",
          "timeToRead": 1,
          "objectID": "6ed6b22c-63cc-5fb2-99d6-eeada5709406",
          "_snippetResult": {
            "text": {
              "value": "\n[Working with JSON data in Standard SQL  |  __ais-highlight__BigQuer__/ais-highlight__y  |  Google Cloud](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/reference/standard-sql/json-data)\n\n先日__ais-highlight__BigQuer__/ais-highlight__yでnative JSON型が",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "February 28, 2022",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "JSONファイルを__ais-highlight__BigQuer__/ais-highlight__yに読ませJSON型で扱うためにそのままCSVで保存する",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/json_to_csv/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\n[Working with JSON data in Standard SQL  |  __ais-highlight__BigQuer__/ais-highlight__y  |  Google Cloud](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/reference/standard-sql/json-data)\n\n先日__ais-highlight__BigQuer__/ais-highlight__yでnative JSON型がプレビューでサポートされた\n\n執筆時点ではloadはCSVしか対応していないようだったのでJSONのファイルはCSVに変換する必要がある\n\nとりあえず使ってみるためにJSONを返すAPIのレスポンスをまるまるCSVにして突っ込んでみることにした\n\n```\ncat hoge.json| jq -r '[.|tostring]|@csv' > hoge.csv\n```\n\n- schema.json\n\n```json\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"data\",\n    \"type\": \"JSON\"\n  }\n]\n```\n\n### load\n\n```\nbq load --replace --source_format=CSV ${GOOGLE_PROJECT}:sample.content_text hoge.csv ./schema.json\n```\n\nこれでJSON型を使えるようになった\n\n1ファイル1レコードという力技だが扱う容量が多くなければこの方法でもいける\n\n何度かSQLたたいてみたけどSTRUCTやREPEATEDなど今まで型でサポートしてくれていた部分を考慮してあげないといけない\n\nなのでいったん特定のカラムを抜き出す工程みたいなのが必要\n\n配列も`JSON_QUERY_ARRAY`を挟んであげる必要があるなどまぁそうだよなという感じ\n\nload対象がJSONファイルで特定のキー以下をJSON型として扱うということができるようになってほしいと感じた\n\n現状だと上記のようにひと手間かけないといけないのでデータレイク的なところから一次整形処理を挟む必要が出てくるのでまだちょっと使いづらいなーという感じ",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "May 07, 2021",
          "title": "PocketのデータをAPI経由でBigQueryに取り込む",
          "slug": "/entries/start_pocket_api/",
          "text": "\nまず`My Applications`から`CREATE APP`でアプリケーションを作成して`consumer key`を取得する\n\n取得した`consumer key`を環境変数に入れておく\n\n```shell\n$ export CONSUMER_KEY=xxxxx\n```\n\n## request tokenの発行\n\n適当なリダイレクト先を指定してrequest tokenを生成する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\n   https://getpocket.com/v3/oauth/request \\\n   -d @-<<EOS\n{\n  \"consumer_key\" : \"${CONSUMER_KEY}\",\n  \"redirect_uri\":\"http://localhost:8001/\"\n}\nEOS\ncode=xxxxx\n```\n\n結果を環境変数に入れておく\n\n```shell\n$ export REQUEST_TOKEN=xxxxx\n```\n\n## ブラウザへ遷移してアプリケーションのアクセス許可を行う\n\nリダイレクト先は適当に\n\n```shell\nopen \"https://getpocket.com/auth/authorize?request_token=${REQUEST_TOKEN}&redirect_uri=http://localhost:8001/\"\n```\n\n## access tokenの発行\n\n先の手順で得たrequest tokenを用いてaccess tokenの発行する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/oauth/authorize \\\n-d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"code\":\"${REQUEST_TOKEN}\"\n}\nEOS\naccess_token=xxxxx&username=hoge\n```\n\n`access_token=`の部分を環境変数に入れておく\n\n```shell\n$ export ACCESS_TOKEN=xxxxx\n```\n\nこれで準備が完了した\n\n## 何かしら問い合わせてみる\n\n記事データを取得してみる\n\n```shell\ncurl -o res.json -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/get -d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"access_token\":\"${ACCESS_TOKEN}\",\n  \"state\":\"unread\",\n  \"detailType\":\"complete\",\n  \"count\":3\n}\nEOS\n```\n\n[Pocket API: Retrieving a User's Pocket Data](https://getpocket.com/developer/docs/v3/retrieve)\n\nretrieveのAPIの仕様についてはこの辺\n\n## おまけ\n\nここで得たJSONをBigQueryに放り込んでよしなにやろうとしたが一筋縄では行かなかった\n\n次のエラーはレスポンスのJSONファイルをそのままGCSにあげて`bq load`しようとした結果\n\n```\nError in query string: Error processing job 'project-111111:bqjob_r75b06933ac2f4481_0000017942c36b05_1': Invalid field name \"3292257344\". Fields must contain only letters, numbers, and\nunderscores, start with a letter or underscore, and be at most 300 characters long. Table: sample_8bb5a901_3d95_41f4_9512_e7f4fad8a737_source\n```\n\nエラー文言自体は`文字またはアンダースコアで始まり`の部分に違反しているのでエラーがでているがそもそもこのキーがIDなので記事によって可変であるためスキーマ定義ができない\n\njson形式が微妙すぎるのでどうしてもフォーマットしてあげないとダメそう\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": {\n    \"3324677936\": {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    \"3324677937\": {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n```\n\nこんな感じで数値キーのハッシュとして出力されている\n\n配列で表現してほしかった…\n\nということで数値キーになっている要素を数値キーを削除した形で保持させる\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": [\n    {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n  ]\n```\n\nこんな感じ\n\n中身を見た感じ`.list`以外にも同様の形式だったのでそちらも同様に配列に変更する必要がある\n\n### ハッシュ→配列にする必要がある要素\n\n執筆時点で把握しているのは下記\n\n- .list\n- .list.images\n- .list.videos\n- .list.authors\n\n### jqでよしなにやる\n\n```\ncat res.json| jq  -cr '.list=(.list|to_entries|map(.value)|map(.images=if has(\"images\") then .images|to_entries|map(.value) else [] end)|map(.videos=if has(\"videos\") then .videos|to_entries|map(.value) else [] end)|map(.authors=if has(\"authors\") then .authors|to_entries|map(.value) else [] end))' > list.json\n```\n\nキー自体がそもそもない場合もあったのでその場合は空配列にする\n\n### BigQueryに入れ込む\n\n```\nbq load --replace --autodetect --source_format=NEWLINE_DELIMITED_JSON sample_dataset.sample list.json\n```\n\nこれでOK",
          "timeToRead": 3,
          "objectID": "196c4775-45c9-57b9-a004-cefc6bc4752e",
          "_snippetResult": {
            "text": {
              "value": "てはこの辺\n\n## おまけ\n\nここで得たJSONを__ais-highlight__BigQuer__/ais-highlight__yに放り込んでよしなにやろうと",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "May 07, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "PocketのデータをAPI経由で__ais-highlight__BigQuer__/ais-highlight__yに取り込む",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/start_pocket_api/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\nまず`My Applications`から`CREATE APP`でアプリケーションを作成して`consumer key`を取得する\n\n取得した`consumer key`を環境変数に入れておく\n\n```shell\n$ export CONSUMER_KEY=xxxxx\n```\n\n## request tokenの発行\n\n適当なリダイレクト先を指定してrequest tokenを生成する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\n   https://getpocket.com/v3/oauth/request \\\n   -d @-<<EOS\n{\n  \"consumer_key\" : \"${CONSUMER_KEY}\",\n  \"redirect_uri\":\"http://localhost:8001/\"\n}\nEOS\ncode=xxxxx\n```\n\n結果を環境変数に入れておく\n\n```shell\n$ export REQUEST_TOKEN=xxxxx\n```\n\n## ブラウザへ遷移してアプリケーションのアクセス許可を行う\n\nリダイレクト先は適当に\n\n```shell\nopen \"https://getpocket.com/auth/authorize?request_token=${REQUEST_TOKEN}&redirect_uri=http://localhost:8001/\"\n```\n\n## access tokenの発行\n\n先の手順で得たrequest tokenを用いてaccess tokenの発行する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/oauth/authorize \\\n-d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"code\":\"${REQUEST_TOKEN}\"\n}\nEOS\naccess_token=xxxxx&username=hoge\n```\n\n`access_token=`の部分を環境変数に入れておく\n\n```shell\n$ export ACCESS_TOKEN=xxxxx\n```\n\nこれで準備が完了した\n\n## 何かしら問い合わせてみる\n\n記事データを取得してみる\n\n```shell\ncurl -o res.json -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/get -d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"access_token\":\"${ACCESS_TOKEN}\",\n  \"state\":\"unread\",\n  \"detailType\":\"complete\",\n  \"count\":3\n}\nEOS\n```\n\n[Pocket API: Retrieving a User's Pocket Data](https://getpocket.com/developer/docs/v3/retrieve)\n\nretrieveのAPIの仕様についてはこの辺\n\n## おまけ\n\nここで得たJSONを__ais-highlight__BigQuer__/ais-highlight__yに放り込んでよしなにやろうとしたが一筋縄では行かなかった\n\n次のエラーはレスポンスのJSONファイルをそのままGCSにあげて`bq load`しようとした結果\n\n```\nError in query string: Error processing job 'project-111111:bqjob_r75b06933ac2f4481_0000017942c36b05_1': Invalid field name \"3292257344\". Fields must contain only letters, numbers, and\nunderscores, start with a letter or underscore, and be at most 300 characters long. Table: sample_8bb5a901_3d95_41f4_9512_e7f4fad8a737_source\n```\n\nエラー文言自体は`文字またはアンダースコアで始まり`の部分に違反しているのでエラーがでているがそもそもこのキーがIDなので記事によって可変であるためスキーマ定義ができない\n\njson形式が微妙すぎるのでどうしてもフォーマットしてあげないとダメそう\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": {\n    \"3324677936\": {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    \"3324677937\": {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n```\n\nこんな感じで数値キーのハッシュとして出力されている\n\n配列で表現してほしかった…\n\nということで数値キーになっている要素を数値キーを削除した形で保持させる\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": [\n    {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n  ]\n```\n\nこんな感じ\n\n中身を見た感じ`.list`以外にも同様の形式だったのでそちらも同様に配列に変更する必要がある\n\n### ハッシュ→配列にする必要がある要素\n\n執筆時点で把握しているのは下記\n\n- .list\n- .list.images\n- .list.videos\n- .list.authors\n\n### jqでよしなにやる\n\n```\ncat res.json| jq  -cr '.list=(.list|to_entries|map(.value)|map(.images=if has(\"images\") then .images|to_entries|map(.value) else [] end)|map(.videos=if has(\"videos\") then .videos|to_entries|map(.value) else [] end)|map(.authors=if has(\"authors\") then .authors|to_entries|map(.value) else [] end))' > list.json\n```\n\nキー自体がそもそもない場合もあったのでその場合は空配列にする\n\n### __ais-highlight__BigQuer__/ais-highlight__yに入れ込む\n\n```\nbq load --replace --autodetect --source_format=NEWLINE_DELIMITED_JSON sample_dataset.sample list.json\n```\n\nこれでOK",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "3",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "July 13, 2021",
          "title": "Workflowsで Memory usage limit exeeded",
          "slug": "/entries/workflows_logging_bigquery_failed/",
          "text": "\nはてなブログのAPIなど公開のAPIをたたいてそのレスポンスをそのままBigQueryに突っ込むみたいなやつ\n\nプライベートなのと規模感が小さいのでちょっと冒険的な感じでやってみようと次のような構成で試みた\n\n- Workflowsではてなブックマークの公開APIをたたく\n- WorkflowsのログにAPIのレスポンスをそのまま流す\n- Logging→集約シンクでBigQueryにレコードを追加する\n\nFunctionsを新たに作らなくても良いしとりあえずの生データも保存できるしわりと省力で実現できるかと考えた\n\nある程度動作確認して問題なかったので自分のブログの全URLで実行したら次のようなエラーが出てしまった\n\n```shell\nExecution failed or cancelled.\nin step \"call_workflow_api\", routine \"call_workflow\", line: 88\nin step \"collect_hatena_bookmark_workflow\", routine \"main\", line: 35\n{\n  \"message\": \"Execution failed or cancelled.\",\n  \"operation\": {\n    \"argument\": \"{\\\"target_url\\\":\\\"https://swfz.hatenablog.com/entry/2018/12/22/080733\\\"}\",\n    \"endTime\": \"2021-07-10T12:01:03.749667278Z\",\n    \"error\": {\n      \"payload\": \"{\\\"message\\\":\\\"ResourceLimitError: Memory usage limit exceeded\\\",\\\"tags\\\":[\\\"ResourceLimitError\\\"]}\",\n      \"stackTrace\": {}\n    },\n    \"name\": \"projects/1111111111111/locations/us-central1/workflows/collect_hatena_bookmark_metrics/executions/c4a686eb-4d92-4e95-94f6-4257438131e0\",\n    \"startTime\": \"2021-07-10T12:01:02.693637032Z\",\n    \"state\": \"FAILED\",\n    \"workflowRevisionId\": \"000001-331\"\n  },\n  \"tags\": [\n    \"OperationError\"\n  ]\n}\n```\n\nバズって300前後のブックマークが付いたURLのレスポンスで発生した\n\n[割り当てと上限  |  ワークフロー  |  Google Cloud](https://cloud.google.com/workflows/quotas)\n\n変数のメモリ割り当てにも上限があり64KBまでらしい\n\nなのでAPIのレスポンスが64KB以上ある場合はエラーになってしまう…\n\nFunctionsは経由するがFunctionsからLoggingへ直接流すようにするか?と思ったが\n\n[割り当てと上限  |  Cloud Logging  |  Google Cloud](https://cloud.google.com/logging/quotas?hl=ja)\n\n同じようにLoggingにも割り当て上限があるのでこの辺も考慮できていないといけない\n\nこの辺まで調べて面倒になってきてしまいこの手法は諦めた\n\nメモリリミットに達してしまったため回避方法はなさそう…APIのレスポンスをそのままWorkflows上でよしなにやるパターンは厳しいという結論になりました\n\nWorkflowsはあくまで各処理のオーケストレーションなのでWorkflows内にあまり処理を持ち込むべきではないっていう考え方なのかなと推測\n\n結局こういうパターンはFunctionsでGCSにデータ置く+BigQueryへloadってパターンがベターなのかな",
          "timeToRead": 2,
          "objectID": "18e04e5f-00f2-50a2-a8d7-b7ac41718457",
          "_snippetResult": {
            "text": {
              "value": "たいてそのレスポンスをそのまま__ais-highlight__BigQuer__/ais-highlight__yに突っ込むみたいなやつ\n\nプライ",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "July 13, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "Workflowsで Memory usage limit exeeded",
              "matchLevel": "none",
              "matchedWords": []
            },
            "slug": {
              "value": "/entries/workflows_logging___ais-highlight__bigquer__/ais-highlight__y_failed/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nはてなブログのAPIなど公開のAPIをたたいてそのレスポンスをそのまま__ais-highlight__BigQuer__/ais-highlight__yに突っ込むみたいなやつ\n\nプライベートなのと規模感が小さいのでちょっと冒険的な感じでやってみようと次のような構成で試みた\n\n- Workflowsではてなブックマークの公開APIをたたく\n- WorkflowsのログにAPIのレスポンスをそのまま流す\n- Logging→集約シンクで__ais-highlight__BigQuer__/ais-highlight__yにレコードを追加する\n\nFunctionsを新たに作らなくても良いしとりあえずの生データも保存できるしわりと省力で実現できるかと考えた\n\nある程度動作確認して問題なかったので自分のブログの全URLで実行したら次のようなエラーが出てしまった\n\n```shell\nExecution failed or cancelled.\nin step \"call_workflow_api\", routine \"call_workflow\", line: 88\nin step \"collect_hatena_bookmark_workflow\", routine \"main\", line: 35\n{\n  \"message\": \"Execution failed or cancelled.\",\n  \"operation\": {\n    \"argument\": \"{\\\"target_url\\\":\\\"https://swfz.hatenablog.com/entry/2018/12/22/080733\\\"}\",\n    \"endTime\": \"2021-07-10T12:01:03.749667278Z\",\n    \"error\": {\n      \"payload\": \"{\\\"message\\\":\\\"ResourceLimitError: Memory usage limit exceeded\\\",\\\"tags\\\":[\\\"ResourceLimitError\\\"]}\",\n      \"stackTrace\": {}\n    },\n    \"name\": \"projects/1111111111111/locations/us-central1/workflows/collect_hatena_bookmark_metrics/executions/c4a686eb-4d92-4e95-94f6-4257438131e0\",\n    \"startTime\": \"2021-07-10T12:01:02.693637032Z\",\n    \"state\": \"FAILED\",\n    \"workflowRevisionId\": \"000001-331\"\n  },\n  \"tags\": [\n    \"OperationError\"\n  ]\n}\n```\n\nバズって300前後のブックマークが付いたURLのレスポンスで発生した\n\n[割り当てと上限  |  ワークフロー  |  Google Cloud](https://cloud.google.com/workflows/quotas)\n\n変数のメモリ割り当てにも上限があり64KBまでらしい\n\nなのでAPIのレスポンスが64KB以上ある場合はエラーになってしまう…\n\nFunctionsは経由するがFunctionsからLoggingへ直接流すようにするか?と思ったが\n\n[割り当てと上限  |  Cloud Logging  |  Google Cloud](https://cloud.google.com/logging/quotas?hl=ja)\n\n同じようにLoggingにも割り当て上限があるのでこの辺も考慮できていないといけない\n\nこの辺まで調べて面倒になってきてしまいこの手法は諦めた\n\nメモリリミットに達してしまったため回避方法はなさそう…APIのレスポンスをそのままWorkflows上でよしなにやるパターンは厳しいという結論になりました\n\nWorkflowsはあくまで各処理のオーケストレーションなのでWorkflows内にあまり処理を持ち込むべきではないっていう考え方なのかなと推測\n\n結局こういうパターンはFunctionsでGCSにデータ置く+__ais-highlight__BigQuer__/ais-highlight__yへloadってパターンがベターなのかな",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "2",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "April 10, 2021",
          "title": "DataPortalで9時区切りの日付カラムを計算フィールドで作成する",
          "slug": "/entries/dataportal_experience_date/",
          "text": "\nただのメモ\n\nTogglからBigQueryにデータを突っ込んでいてそれをDataPortal経由でグラフ化している\n\n`start: 2020-12-25 21:52:30 UTC`（実際計測している時刻UTCではなくAsia/TokyoだがToggl側のAPIが返す値は時刻+UTCという値が返ってきている）\n\n上記のようなフォーマットのカラムを午前9時を堺にグルーピングしたいという要件が出てきた\n\n## 経緯\n\n単にグラフ化した場合`start`を基準にして日付単位でグループ化すると\n\n睡眠時間によっては日の合計時間が24時間を超えてしまうためグラフを眺めていて違和感がある\n\nそのため現在は0時をまたいで睡眠をとった場合は0時を境に分割して記録している\n\n```\n睡眠: 2021-04-09 22:00:00 ～ 2021-04-10 07:00:00\nToggl上での記録: \n- 2021-04-09 22:00:00 ～ 2021-04-10 00:00:00\n- 2021-04-10 00:00:00 ～ 2021-04-10 07:00:00\n```\n\nそうすると正確な日付としては分割して結果を閲覧できるが自分の体感としての日の睡眠時間がずれてグラフ化されてしまう\n\nそこで、冒頭のように午前9時開始を堺に日付を分割して0-9時のデータは前日分としてグラフ上では扱えるようにする\n\nそのための計算フィールドの計算式が下記\n\n```sql\nif(\n  parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT%H\",start)\n  ) < parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT09\",start)\n  ),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", datetime_sub(start, INTERVAL 1 DAY))),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", start))\n)\n```\n\n## つまずきポイント\n- DataPortalで日付カラムとして扱う場合はDate型かDateTime型になっている必要があるので結果に`parse_date`などパース処理が必要\n- `start`自体も日付・時刻カラムだったのでまず`format_datetime`でフォーマットしてからさらに`parse_datetime`で比較させて上げる必要があった\n- よく考えれば分かるはずだったがDataPortal上のテーブルで可視化すると別のフォーマットで出力されてしまい若干混乱した\n\nこんな感じで午前9時を境に日付を変更できた\n\n![alt](dataportal_experience_date01.png)",
          "timeToRead": 2,
          "objectID": "17cb94a3-6fe4-5894-bec2-e090afbbdb8d",
          "_snippetResult": {
            "text": {
              "value": "\nただのメモ\n\nTogglから__ais-highlight__BigQuer__/ais-highlight__yにデータを突っ込んでいてそれをDataPortal経由でグ",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "April 10, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "DataPortalで9時区切りの日付カラムを計算フィールドで作成する",
              "matchLevel": "none",
              "matchedWords": []
            },
            "slug": {
              "value": "/entries/dataportal_experience_date/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\nただのメモ\n\nTogglから__ais-highlight__BigQuer__/ais-highlight__yにデータを突っ込んでいてそれをDataPortal経由でグラフ化している\n\n`start: 2020-12-25 21:52:30 UTC`（実際計測している時刻UTCではなくAsia/TokyoだがToggl側のAPIが返す値は時刻+UTCという値が返ってきている）\n\n上記のようなフォーマットのカラムを午前9時を堺にグルーピングしたいという要件が出てきた\n\n## 経緯\n\n単にグラフ化した場合`start`を基準にして日付単位でグループ化すると\n\n睡眠時間によっては日の合計時間が24時間を超えてしまうためグラフを眺めていて違和感がある\n\nそのため現在は0時をまたいで睡眠をとった場合は0時を境に分割して記録している\n\n```\n睡眠: 2021-04-09 22:00:00 ～ 2021-04-10 07:00:00\nToggl上での記録: \n- 2021-04-09 22:00:00 ～ 2021-04-10 00:00:00\n- 2021-04-10 00:00:00 ～ 2021-04-10 07:00:00\n```\n\nそうすると正確な日付としては分割して結果を閲覧できるが自分の体感としての日の睡眠時間がずれてグラフ化されてしまう\n\nそこで、冒頭のように午前9時開始を堺に日付を分割して0-9時のデータは前日分としてグラフ上では扱えるようにする\n\nそのための計算フィールドの計算式が下記\n\n```sql\nif(\n  parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT%H\",start)\n  ) < parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT09\",start)\n  ),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", datetime_sub(start, INTERVAL 1 DAY))),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", start))\n)\n```\n\n## つまずきポイント\n- DataPortalで日付カラムとして扱う場合はDate型かDateTime型になっている必要があるので結果に`parse_date`などパース処理が必要\n- `start`自体も日付・時刻カラムだったのでまず`format_datetime`でフォーマットしてからさらに`parse_datetime`で比較させて上げる必要があった\n- よく考えれば分かるはずだったがDataPortal上のテーブルで可視化すると別のフォーマットで出力されてしまい若干混乱した\n\nこんな感じで午前9時を境に日付を変更できた\n\n![alt](dataportal_experience_date01.png)",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "2",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        }
      ],
      "nbHits": 8,
      "page": 0,
      "nbPages": 1,
      "hitsPerPage": 20,
      "exhaustiveNbHits": true,
      "exhaustiveTypo": true,
      "query": "BigQuer",
      "params": "facets=%5B%5D&highlightPostTag=__%2Fais-highlight__&highlightPreTag=__ais-highlight__&query=BigQuer&tagFilters=",
      "index": "til",
      "renderingContent": {},
      "processingTimeMS": 3
    },
    {
      "hits": [
        {
          "date": "March 25, 2022",
          "title": "BigQueryの日付を扱う際のメモ",
          "slug": "/entries/bigquery_date_function/",
          "text": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day, # 月初\nLAST_DAY(CURRENT_DATE(), MONTH) AS last_day, # 月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS first_day_of_yesterday, # 前日起算の月初\nLAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS last_day_of_yesterday, # 前日起算の月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH), MONTH) AS first_day_of_last_three_month # 3ヶ月前の月初\n```\n\nDATE_TRUNC, LAST_DAYが便利",
          "timeToRead": 1,
          "objectID": "68f46908-591f-5bb4-82bd-f2fc099406d2",
          "_snippetResult": {
            "text": {
              "value": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day",
              "matchLevel": "none"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "March 25, 2022",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yの日付を扱う際のメモ",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_date_function/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nよく使うと思われるクエリをメモしておく\n\n```sql\nSELECT\nDATE_TRUNC(CURRENT_DATE(), MONTH) AS first_day, # 月初\nLAST_DAY(CURRENT_DATE(), MONTH) AS last_day, # 月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS first_day_of_yesterday, # 前日起算の月初\nLAST_DAY(DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY), MONTH) AS last_day_of_yesterday, # 前日起算の月末\nDATE_TRUNC(DATE_SUB(CURRENT_DATE(), INTERVAL 3 MONTH), MONTH) AS first_day_of_last_three_month # 3ヶ月前の月初\n```\n\nDATE_TRUNC, LAST_DAYが便利",
              "matchLevel": "none",
              "matchedWords": []
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "May 08, 2021",
          "title": "BigQueryのbq load時にautodetectを使えない場合",
          "slug": "/entries/bigquery_cant_use_autodetect/",
          "text": "\nPocketのデータをAPIで取得してBigQueryに突っ込もうとしたときの話\n\nGCSにJSONを置いてCLIから`bq load --autodetect`でデータをloadしようとしたらエラーで怒られた\n\n```\nBigQuery error in load operation: Error processing job\n'project-111111:bqjob_r70118be7bda78ce4_000001793f9c2946_1': Error while reading\ndata, error message: JSON table encountered too many errors, giving up. Rows: 1;\nerrors: 1. Please look into the errors[] collection for more details.\nFailure details:\n- Error while reading data, error message: JSON processing\nencountered too many errors, giving up. Rows: 1; errors: 1; max\nbad: 0; error percent: 0\n- gs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json: Error\nwhile reading data, error message: JSON parsing error in row\nstarting at position 0: JSON object specified for non-record field:\nlist.videos\n```\n\n## 前提\n\n現状あるデータは次の3つ（bucket名はサンプル）\n\n```\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-03.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-05.json\n```\n\n## 原因の切り分け\n\n- raw-03.json\n- raw-04.json\n\nのときは問題なくloadできている\n\n`raw-05.json`\n\nが追加されてから上記エラーになってしまった\n\n05と03,04のJSONの中身を比べてみたところ05には`list.videos`がすべて`[]`になっていた\n\n03,04に関してはどこかのレコードでオブジェクトが入っていたので`RECORD`と判断された模様\n\nこのことから`--autodetect`は最初のファイルをautodetectで読み込んで順番にその他ファイルも読み込んでいると考えられる\n\n[スキーマの自動検出](https://cloud.google.com/bigquery/docs/schema-detect?hl=ja#auto-detect)\n\nここに説明が書いてあった\n\n> 自動検出を有効にすると、BigQuery はデータソース内でランダムにファイルを選択します。ファイルの最大 100 行をスキャンして代表的なサンプルとして使用し、推定プロセスを開始します。BigQuery は、各フィールドを検証し、そのサンプル内の値に基づいてそのフィールドにデータ型を割り当てようとします。\n\nランダムでファイルを読み込むとあるので全パターンを網羅したデータがあるファイルじゃないファイルがサンプルに選定されてしまった場合にこういうことが起きる\n\nそうなると`autodetect`は使えないのでテーブル作成→loadの手順を踏む必要がある\n\n- schemaの取り出し\n\nうまく行ったパターンで生成したテーブルのスキーマを取得する\n\n```\nbq show --schema --format=prettyjson pocket.rawdata > pocket-rawdata.json\n```\n\n- テーブル作成\n\n別のテーブルを用意して試してみる\n\n```\nbq mk --table --time_partitioning_field month --time_partitioning_type MONTH sample.pocket_rawdata pocket-rawdata.json\n```\n\n- load\n\n```\nbq load --source_format=NEWLINE_DELIMITED_JSON --replace sample.pocket_rawdata 'gs://sample-bucket/preprocessed_rawdata/*'\n```\n\n今のところこんな感じでなんとかなっている\n\n## まとめ\n\n取り込み対象のデータにばらつきがある（あるデータではRECORD、あるデータでは`[]`のようなとき）とサンプリング次第で取り込めない場合がある\n\nさらに`--autodetect`+`--replace`を用いると毎回ロード時に自動検出するので失敗する可能性がある\n\nそのためスキーマを定義してテーブルの作成+`bq load`と手順を踏む必要がある\n\n## 所感\n\n本来だったら`autodetect`で生成したスキーマからさらに精査して本当に`NULLABLE`?みたいな話も考えたほうが良いが今回は趣味プロジェクトなので…\n\nautodetectは便利だけどこういうパターンに対応できないのでやはりPOCやお試しのときくらいしか使えないよなーとあらためて感じた\n\nまぁでも気軽に試せるのはとても良いことなので使い分けが大事",
          "timeToRead": 3,
          "objectID": "511a0a9b-6cec-55d0-a965-148667fcf789",
          "_snippetResult": {
            "text": {
              "value": "\nPocketのデータをAPIで取得して__ais-highlight__BigQuer__/ais-highlight__yに突っ込もうとしたときの話\n\nGCSにJSON",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "May 08, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yのbq load時にautodetectを使えない場合",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_cant_use_autodetect/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nPocketのデータをAPIで取得して__ais-highlight__BigQuer__/ais-highlight__yに突っ込もうとしたときの話\n\nGCSにJSONを置いてCLIから`bq load --autodetect`でデータをloadしようとしたらエラーで怒られた\n\n```\n__ais-highlight__BigQuer__/ais-highlight__y error in load operation: Error processing job\n'project-111111:bqjob_r70118be7bda78ce4_000001793f9c2946_1': Error while reading\ndata, error message: JSON table encountered too many errors, giving up. Rows: 1;\nerrors: 1. Please look into the errors[] collection for more details.\nFailure details:\n- Error while reading data, error message: JSON processing\nencountered too many errors, giving up. Rows: 1; errors: 1; max\nbad: 0; error percent: 0\n- gs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json: Error\nwhile reading data, error message: JSON parsing error in row\nstarting at position 0: JSON object specified for non-record field:\nlist.videos\n```\n\n## 前提\n\n現状あるデータは次の3つ（bucket名はサンプル）\n\n```\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-03.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-04.json\ngs://sample-bucket/preprocessed_rawdata/month=2021-05-01/raw-05.json\n```\n\n## 原因の切り分け\n\n- raw-03.json\n- raw-04.json\n\nのときは問題なくloadできている\n\n`raw-05.json`\n\nが追加されてから上記エラーになってしまった\n\n05と03,04のJSONの中身を比べてみたところ05には`list.videos`がすべて`[]`になっていた\n\n03,04に関してはどこかのレコードでオブジェクトが入っていたので`RECORD`と判断された模様\n\nこのことから`--autodetect`は最初のファイルをautodetectで読み込んで順番にその他ファイルも読み込んでいると考えられる\n\n[スキーマの自動検出](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/schema-detect?hl=ja#auto-detect)\n\nここに説明が書いてあった\n\n> 自動検出を有効にすると、__ais-highlight__BigQuer__/ais-highlight__y はデータソース内でランダムにファイルを選択します。ファイルの最大 100 行をスキャンして代表的なサンプルとして使用し、推定プロセスを開始します。__ais-highlight__BigQuer__/ais-highlight__y は、各フィールドを検証し、そのサンプル内の値に基づいてそのフィールドにデータ型を割り当てようとします。\n\nランダムでファイルを読み込むとあるので全パターンを網羅したデータがあるファイルじゃないファイルがサンプルに選定されてしまった場合にこういうことが起きる\n\nそうなると`autodetect`は使えないのでテーブル作成→loadの手順を踏む必要がある\n\n- schemaの取り出し\n\nうまく行ったパターンで生成したテーブルのスキーマを取得する\n\n```\nbq show --schema --format=prettyjson pocket.rawdata > pocket-rawdata.json\n```\n\n- テーブル作成\n\n別のテーブルを用意して試してみる\n\n```\nbq mk --table --time_partitioning_field month --time_partitioning_type MONTH sample.pocket_rawdata pocket-rawdata.json\n```\n\n- load\n\n```\nbq load --source_format=NEWLINE_DELIMITED_JSON --replace sample.pocket_rawdata 'gs://sample-bucket/preprocessed_rawdata/*'\n```\n\n今のところこんな感じでなんとかなっている\n\n## まとめ\n\n取り込み対象のデータにばらつきがある（あるデータではRECORD、あるデータでは`[]`のようなとき）とサンプリング次第で取り込めない場合がある\n\nさらに`--autodetect`+`--replace`を用いると毎回ロード時に自動検出するので失敗する可能性がある\n\nそのためスキーマを定義してテーブルの作成+`bq load`と手順を踏む必要がある\n\n## 所感\n\n本来だったら`autodetect`で生成したスキーマからさらに精査して本当に`NULLABLE`?みたいな話も考えたほうが良いが今回は趣味プロジェクトなので…\n\nautodetectは便利だけどこういうパターンに対応できないのでやはりPOCやお試しのときくらいしか使えないよなーとあらためて感じた\n\nまぁでも気軽に試せるのはとても良いことなので使い分けが大事",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "3",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "December 09, 2020",
          "title": "BigQueryでサンプルデータをサクッと作る",
          "slug": "/entries/bigquery_sample_data/",
          "text": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ込まなくてもサンプルデータを生成できる\n\n```sql\nWITH sample AS(\n  SELECT * FROM UNNEST(ARRAY<STRUCT<start_date DATE, end_date DATE, item STRING, sales INT64>>\n    [\n      (\"2020-08-01\", \"2020-11-30\", \"hoge\", 100),\n      (\"2020-10-01\", \"2020-10-31\", \"fuga\", 200)\n    ]\n  )\n)\nSELECT * FROM sample\n```\n\n- 結果\n\n|start_date|end_date|item|sales|\n|---|---|---|---|\n|2020-08-01|2020-11-30|hoge|100|\n|2020-10-01|2020-10-31|fuga|200|\n\n\n記事書くときや説明とかに使える\n",
          "timeToRead": 1,
          "objectID": "28192504-51b0-5f94-9f12-c62f278c23cc",
          "_snippetResult": {
            "text": {
              "value": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ",
              "matchLevel": "none"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "December 09, 2020",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yでサンプルデータをサクッと作る",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_sample_data/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\n簡易的にでもサンプルデータが欲しい場合、わざわざデータを入れ込まなくてもサンプルデータを生成できる\n\n```sql\nWITH sample AS(\n  SELECT * FROM UNNEST(ARRAY<STRUCT<start_date DATE, end_date DATE, item STRING, sales INT64>>\n    [\n      (\"2020-08-01\", \"2020-11-30\", \"hoge\", 100),\n      (\"2020-10-01\", \"2020-10-31\", \"fuga\", 200)\n    ]\n  )\n)\nSELECT * FROM sample\n```\n\n- 結果\n\n|start_date|end_date|item|sales|\n|---|---|---|---|\n|2020-08-01|2020-11-30|hoge|100|\n|2020-10-01|2020-10-31|fuga|200|\n\n\n記事書くときや説明とかに使える\n",
              "matchLevel": "none",
              "matchedWords": []
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "April 21, 2021",
          "title": "BigQueryで日付を扱うときはTimezoneを意識する",
          "slug": "/entries/bigquery_date_timezone/",
          "text": "\ndataformでsourceテーブルから中間テーブルを生成してassertionを書いていた\n\n検算したら件数が合わないなーということで調べた\n\n次のようなSQLで`from`,`to`を指定して単月分のレコードのみ抜き出すというパターン\n\n```sql\nSELECT\n  *,\n  'private' AS workspace\nFROM\n  `sample.rawdata-private`,\n  UNNEST(data) AS d\nWHERE\n  DATE(start) BETWEEN ${target_date.from}\n  AND ${target_date.to}\n```\n\n<!-- textlint-disable prh -->\nSQLXなので`target_date.to`と`target_date.from`はその時々によって変化する\n<!-- textlint-enable prh -->\n\n今回は`2021-04-01` ～ `2021-04-30`をという感じ\n\n`rawdata-private`はAPIのレスポンスをそのまま保存していて1行に`total_count`と`data`列に実際のレコードがあるので`UNNEST`してレコード数と比較することで確認している\n\n`rawdata-private`のレコードを追ってみると\n\n```json\n\"start\": \"2021-04-01T04:57:39+09:00\",\n```\n\nのデータが`DATE(start)`を通すことで`2021-03-31`になっていた\n\nなるほどUTC\n\n`DATE(start, 'Asia/Tokyo'),`でタイムゾーン指定の日付データに変換できるのでこれで対応\n\nBigQueryがDATEでよしなにやってくれた結果UTCで解釈すると`2021-03-31`となってしまうためフィルタ対象から外れてしまい、件数が合わない状態になっていた\n\n正直assertion書いてなかったら気付かなかったのでassertion大事w",
          "timeToRead": 1,
          "objectID": "1d531f2a-2c16-5859-96ce-a8cf37a230b8",
          "_snippetResult": {
            "text": {
              "value": "ータに変換できるのでこれで対応\n\n__ais-highlight__BigQuer__/ais-highlight__yがDATEでよしなにやってくれた結",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "April 21, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "__ais-highlight__BigQuer__/ais-highlight__yで日付を扱うときはTimezoneを意識する",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/__ais-highlight__bigquer__/ais-highlight__y_date_timezone/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\ndataformでsourceテーブルから中間テーブルを生成してassertionを書いていた\n\n検算したら件数が合わないなーということで調べた\n\n次のようなSQLで`from`,`to`を指定して単月分のレコードのみ抜き出すというパターン\n\n```sql\nSELECT\n  *,\n  'private' AS workspace\nFROM\n  `sample.rawdata-private`,\n  UNNEST(data) AS d\nWHERE\n  DATE(start) BETWEEN ${target_date.from}\n  AND ${target_date.to}\n```\n\n<!-- textlint-disable prh -->\nSQLXなので`target_date.to`と`target_date.from`はその時々によって変化する\n<!-- textlint-enable prh -->\n\n今回は`2021-04-01` ～ `2021-04-30`をという感じ\n\n`rawdata-private`はAPIのレスポンスをそのまま保存していて1行に`total_count`と`data`列に実際のレコードがあるので`UNNEST`してレコード数と比較することで確認している\n\n`rawdata-private`のレコードを追ってみると\n\n```json\n\"start\": \"2021-04-01T04:57:39+09:00\",\n```\n\nのデータが`DATE(start)`を通すことで`2021-03-31`になっていた\n\nなるほどUTC\n\n`DATE(start, 'Asia/Tokyo'),`でタイムゾーン指定の日付データに変換できるのでこれで対応\n\n__ais-highlight__BigQuer__/ais-highlight__yがDATEでよしなにやってくれた結果UTCで解釈すると`2021-03-31`となってしまうためフィルタ対象から外れてしまい、件数が合わない状態になっていた\n\n正直assertion書いてなかったら気付かなかったのでassertion大事w",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "February 28, 2022",
          "title": "JSONファイルをBigQueryに読ませJSON型で扱うためにそのままCSVで保存する",
          "slug": "/entries/json_to_csv/",
          "text": "\n[Working with JSON data in Standard SQL  |  BigQuery  |  Google Cloud](https://cloud.google.com/bigquery/docs/reference/standard-sql/json-data)\n\n先日BigQueryでnative JSON型がプレビューでサポートされた\n\n執筆時点ではloadはCSVしか対応していないようだったのでJSONのファイルはCSVに変換する必要がある\n\nとりあえず使ってみるためにJSONを返すAPIのレスポンスをまるまるCSVにして突っ込んでみることにした\n\n```\ncat hoge.json| jq -r '[.|tostring]|@csv' > hoge.csv\n```\n\n- schema.json\n\n```json\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"data\",\n    \"type\": \"JSON\"\n  }\n]\n```\n\n### load\n\n```\nbq load --replace --source_format=CSV ${GOOGLE_PROJECT}:sample.content_text hoge.csv ./schema.json\n```\n\nこれでJSON型を使えるようになった\n\n1ファイル1レコードという力技だが扱う容量が多くなければこの方法でもいける\n\n何度かSQLたたいてみたけどSTRUCTやREPEATEDなど今まで型でサポートしてくれていた部分を考慮してあげないといけない\n\nなのでいったん特定のカラムを抜き出す工程みたいなのが必要\n\n配列も`JSON_QUERY_ARRAY`を挟んであげる必要があるなどまぁそうだよなという感じ\n\nload対象がJSONファイルで特定のキー以下をJSON型として扱うということができるようになってほしいと感じた\n\n現状だと上記のようにひと手間かけないといけないのでデータレイク的なところから一次整形処理を挟む必要が出てくるのでまだちょっと使いづらいなーという感じ",
          "timeToRead": 1,
          "objectID": "6ed6b22c-63cc-5fb2-99d6-eeada5709406",
          "_snippetResult": {
            "text": {
              "value": "\n[Working with JSON data in Standard SQL  |  __ais-highlight__BigQuer__/ais-highlight__y  |  Google Cloud](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/reference/standard-sql/json-data)\n\n先日__ais-highlight__BigQuer__/ais-highlight__yでnative JSON型が",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "February 28, 2022",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "JSONファイルを__ais-highlight__BigQuer__/ais-highlight__yに読ませJSON型で扱うためにそのままCSVで保存する",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/json_to_csv/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\n[Working with JSON data in Standard SQL  |  __ais-highlight__BigQuer__/ais-highlight__y  |  Google Cloud](https://cloud.google.com/__ais-highlight__bigquer__/ais-highlight__y/docs/reference/standard-sql/json-data)\n\n先日__ais-highlight__BigQuer__/ais-highlight__yでnative JSON型がプレビューでサポートされた\n\n執筆時点ではloadはCSVしか対応していないようだったのでJSONのファイルはCSVに変換する必要がある\n\nとりあえず使ってみるためにJSONを返すAPIのレスポンスをまるまるCSVにして突っ込んでみることにした\n\n```\ncat hoge.json| jq -r '[.|tostring]|@csv' > hoge.csv\n```\n\n- schema.json\n\n```json\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"data\",\n    \"type\": \"JSON\"\n  }\n]\n```\n\n### load\n\n```\nbq load --replace --source_format=CSV ${GOOGLE_PROJECT}:sample.content_text hoge.csv ./schema.json\n```\n\nこれでJSON型を使えるようになった\n\n1ファイル1レコードという力技だが扱う容量が多くなければこの方法でもいける\n\n何度かSQLたたいてみたけどSTRUCTやREPEATEDなど今まで型でサポートしてくれていた部分を考慮してあげないといけない\n\nなのでいったん特定のカラムを抜き出す工程みたいなのが必要\n\n配列も`JSON_QUERY_ARRAY`を挟んであげる必要があるなどまぁそうだよなという感じ\n\nload対象がJSONファイルで特定のキー以下をJSON型として扱うということができるようになってほしいと感じた\n\n現状だと上記のようにひと手間かけないといけないのでデータレイク的なところから一次整形処理を挟む必要が出てくるのでまだちょっと使いづらいなーという感じ",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "1",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "May 07, 2021",
          "title": "PocketのデータをAPI経由でBigQueryに取り込む",
          "slug": "/entries/start_pocket_api/",
          "text": "\nまず`My Applications`から`CREATE APP`でアプリケーションを作成して`consumer key`を取得する\n\n取得した`consumer key`を環境変数に入れておく\n\n```shell\n$ export CONSUMER_KEY=xxxxx\n```\n\n## request tokenの発行\n\n適当なリダイレクト先を指定してrequest tokenを生成する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\n   https://getpocket.com/v3/oauth/request \\\n   -d @-<<EOS\n{\n  \"consumer_key\" : \"${CONSUMER_KEY}\",\n  \"redirect_uri\":\"http://localhost:8001/\"\n}\nEOS\ncode=xxxxx\n```\n\n結果を環境変数に入れておく\n\n```shell\n$ export REQUEST_TOKEN=xxxxx\n```\n\n## ブラウザへ遷移してアプリケーションのアクセス許可を行う\n\nリダイレクト先は適当に\n\n```shell\nopen \"https://getpocket.com/auth/authorize?request_token=${REQUEST_TOKEN}&redirect_uri=http://localhost:8001/\"\n```\n\n## access tokenの発行\n\n先の手順で得たrequest tokenを用いてaccess tokenの発行する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/oauth/authorize \\\n-d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"code\":\"${REQUEST_TOKEN}\"\n}\nEOS\naccess_token=xxxxx&username=hoge\n```\n\n`access_token=`の部分を環境変数に入れておく\n\n```shell\n$ export ACCESS_TOKEN=xxxxx\n```\n\nこれで準備が完了した\n\n## 何かしら問い合わせてみる\n\n記事データを取得してみる\n\n```shell\ncurl -o res.json -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/get -d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"access_token\":\"${ACCESS_TOKEN}\",\n  \"state\":\"unread\",\n  \"detailType\":\"complete\",\n  \"count\":3\n}\nEOS\n```\n\n[Pocket API: Retrieving a User's Pocket Data](https://getpocket.com/developer/docs/v3/retrieve)\n\nretrieveのAPIの仕様についてはこの辺\n\n## おまけ\n\nここで得たJSONをBigQueryに放り込んでよしなにやろうとしたが一筋縄では行かなかった\n\n次のエラーはレスポンスのJSONファイルをそのままGCSにあげて`bq load`しようとした結果\n\n```\nError in query string: Error processing job 'project-111111:bqjob_r75b06933ac2f4481_0000017942c36b05_1': Invalid field name \"3292257344\". Fields must contain only letters, numbers, and\nunderscores, start with a letter or underscore, and be at most 300 characters long. Table: sample_8bb5a901_3d95_41f4_9512_e7f4fad8a737_source\n```\n\nエラー文言自体は`文字またはアンダースコアで始まり`の部分に違反しているのでエラーがでているがそもそもこのキーがIDなので記事によって可変であるためスキーマ定義ができない\n\njson形式が微妙すぎるのでどうしてもフォーマットしてあげないとダメそう\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": {\n    \"3324677936\": {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    \"3324677937\": {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n```\n\nこんな感じで数値キーのハッシュとして出力されている\n\n配列で表現してほしかった…\n\nということで数値キーになっている要素を数値キーを削除した形で保持させる\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": [\n    {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n  ]\n```\n\nこんな感じ\n\n中身を見た感じ`.list`以外にも同様の形式だったのでそちらも同様に配列に変更する必要がある\n\n### ハッシュ→配列にする必要がある要素\n\n執筆時点で把握しているのは下記\n\n- .list\n- .list.images\n- .list.videos\n- .list.authors\n\n### jqでよしなにやる\n\n```\ncat res.json| jq  -cr '.list=(.list|to_entries|map(.value)|map(.images=if has(\"images\") then .images|to_entries|map(.value) else [] end)|map(.videos=if has(\"videos\") then .videos|to_entries|map(.value) else [] end)|map(.authors=if has(\"authors\") then .authors|to_entries|map(.value) else [] end))' > list.json\n```\n\nキー自体がそもそもない場合もあったのでその場合は空配列にする\n\n### BigQueryに入れ込む\n\n```\nbq load --replace --autodetect --source_format=NEWLINE_DELIMITED_JSON sample_dataset.sample list.json\n```\n\nこれでOK",
          "timeToRead": 3,
          "objectID": "196c4775-45c9-57b9-a004-cefc6bc4752e",
          "_snippetResult": {
            "text": {
              "value": "てはこの辺\n\n## おまけ\n\nここで得たJSONを__ais-highlight__BigQuer__/ais-highlight__yに放り込んでよしなにやろうと",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "May 07, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "PocketのデータをAPI経由で__ais-highlight__BigQuer__/ais-highlight__yに取り込む",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "slug": {
              "value": "/entries/start_pocket_api/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\nまず`My Applications`から`CREATE APP`でアプリケーションを作成して`consumer key`を取得する\n\n取得した`consumer key`を環境変数に入れておく\n\n```shell\n$ export CONSUMER_KEY=xxxxx\n```\n\n## request tokenの発行\n\n適当なリダイレクト先を指定してrequest tokenを生成する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\n   https://getpocket.com/v3/oauth/request \\\n   -d @-<<EOS\n{\n  \"consumer_key\" : \"${CONSUMER_KEY}\",\n  \"redirect_uri\":\"http://localhost:8001/\"\n}\nEOS\ncode=xxxxx\n```\n\n結果を環境変数に入れておく\n\n```shell\n$ export REQUEST_TOKEN=xxxxx\n```\n\n## ブラウザへ遷移してアプリケーションのアクセス許可を行う\n\nリダイレクト先は適当に\n\n```shell\nopen \"https://getpocket.com/auth/authorize?request_token=${REQUEST_TOKEN}&redirect_uri=http://localhost:8001/\"\n```\n\n## access tokenの発行\n\n先の手順で得たrequest tokenを用いてaccess tokenの発行する\n\n```shell\n$ curl -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/oauth/authorize \\\n-d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"code\":\"${REQUEST_TOKEN}\"\n}\nEOS\naccess_token=xxxxx&username=hoge\n```\n\n`access_token=`の部分を環境変数に入れておく\n\n```shell\n$ export ACCESS_TOKEN=xxxxx\n```\n\nこれで準備が完了した\n\n## 何かしら問い合わせてみる\n\n記事データを取得してみる\n\n```shell\ncurl -o res.json -H \"Content-Type: application/json; charset=UTF-8\" -X POST \\\nhttps://getpocket.com/v3/get -d @-<<EOS\n{\n  \"consumer_key\":\"${CONSUMER_KEY}\",\n  \"access_token\":\"${ACCESS_TOKEN}\",\n  \"state\":\"unread\",\n  \"detailType\":\"complete\",\n  \"count\":3\n}\nEOS\n```\n\n[Pocket API: Retrieving a User's Pocket Data](https://getpocket.com/developer/docs/v3/retrieve)\n\nretrieveのAPIの仕様についてはこの辺\n\n## おまけ\n\nここで得たJSONを__ais-highlight__BigQuer__/ais-highlight__yに放り込んでよしなにやろうとしたが一筋縄では行かなかった\n\n次のエラーはレスポンスのJSONファイルをそのままGCSにあげて`bq load`しようとした結果\n\n```\nError in query string: Error processing job 'project-111111:bqjob_r75b06933ac2f4481_0000017942c36b05_1': Invalid field name \"3292257344\". Fields must contain only letters, numbers, and\nunderscores, start with a letter or underscore, and be at most 300 characters long. Table: sample_8bb5a901_3d95_41f4_9512_e7f4fad8a737_source\n```\n\nエラー文言自体は`文字またはアンダースコアで始まり`の部分に違反しているのでエラーがでているがそもそもこのキーがIDなので記事によって可変であるためスキーマ定義ができない\n\njson形式が微妙すぎるのでどうしてもフォーマットしてあげないとダメそう\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": {\n    \"3324677936\": {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    \"3324677937\": {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n```\n\nこんな感じで数値キーのハッシュとして出力されている\n\n配列で表現してほしかった…\n\nということで数値キーになっている要素を数値キーを削除した形で保持させる\n\n```json\n{\n  \"status\": 1,\n  \"complete\": 0,\n  \"list\": [\n    {\n      \"item_id\": \"3324677936\",\n      \"resolved_id\": \"3324677936\",\n      .....\n    },\n    {\n      \"item_id\": \"3324677937\",\n      \"resolved_id\": \"3324677937\",\n      .....\n    },\n    .....\n  ]\n```\n\nこんな感じ\n\n中身を見た感じ`.list`以外にも同様の形式だったのでそちらも同様に配列に変更する必要がある\n\n### ハッシュ→配列にする必要がある要素\n\n執筆時点で把握しているのは下記\n\n- .list\n- .list.images\n- .list.videos\n- .list.authors\n\n### jqでよしなにやる\n\n```\ncat res.json| jq  -cr '.list=(.list|to_entries|map(.value)|map(.images=if has(\"images\") then .images|to_entries|map(.value) else [] end)|map(.videos=if has(\"videos\") then .videos|to_entries|map(.value) else [] end)|map(.authors=if has(\"authors\") then .authors|to_entries|map(.value) else [] end))' > list.json\n```\n\nキー自体がそもそもない場合もあったのでその場合は空配列にする\n\n### __ais-highlight__BigQuer__/ais-highlight__yに入れ込む\n\n```\nbq load --replace --autodetect --source_format=NEWLINE_DELIMITED_JSON sample_dataset.sample list.json\n```\n\nこれでOK",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "3",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "July 13, 2021",
          "title": "Workflowsで Memory usage limit exeeded",
          "slug": "/entries/workflows_logging_bigquery_failed/",
          "text": "\nはてなブログのAPIなど公開のAPIをたたいてそのレスポンスをそのままBigQueryに突っ込むみたいなやつ\n\nプライベートなのと規模感が小さいのでちょっと冒険的な感じでやってみようと次のような構成で試みた\n\n- Workflowsではてなブックマークの公開APIをたたく\n- WorkflowsのログにAPIのレスポンスをそのまま流す\n- Logging→集約シンクでBigQueryにレコードを追加する\n\nFunctionsを新たに作らなくても良いしとりあえずの生データも保存できるしわりと省力で実現できるかと考えた\n\nある程度動作確認して問題なかったので自分のブログの全URLで実行したら次のようなエラーが出てしまった\n\n```shell\nExecution failed or cancelled.\nin step \"call_workflow_api\", routine \"call_workflow\", line: 88\nin step \"collect_hatena_bookmark_workflow\", routine \"main\", line: 35\n{\n  \"message\": \"Execution failed or cancelled.\",\n  \"operation\": {\n    \"argument\": \"{\\\"target_url\\\":\\\"https://swfz.hatenablog.com/entry/2018/12/22/080733\\\"}\",\n    \"endTime\": \"2021-07-10T12:01:03.749667278Z\",\n    \"error\": {\n      \"payload\": \"{\\\"message\\\":\\\"ResourceLimitError: Memory usage limit exceeded\\\",\\\"tags\\\":[\\\"ResourceLimitError\\\"]}\",\n      \"stackTrace\": {}\n    },\n    \"name\": \"projects/1111111111111/locations/us-central1/workflows/collect_hatena_bookmark_metrics/executions/c4a686eb-4d92-4e95-94f6-4257438131e0\",\n    \"startTime\": \"2021-07-10T12:01:02.693637032Z\",\n    \"state\": \"FAILED\",\n    \"workflowRevisionId\": \"000001-331\"\n  },\n  \"tags\": [\n    \"OperationError\"\n  ]\n}\n```\n\nバズって300前後のブックマークが付いたURLのレスポンスで発生した\n\n[割り当てと上限  |  ワークフロー  |  Google Cloud](https://cloud.google.com/workflows/quotas)\n\n変数のメモリ割り当てにも上限があり64KBまでらしい\n\nなのでAPIのレスポンスが64KB以上ある場合はエラーになってしまう…\n\nFunctionsは経由するがFunctionsからLoggingへ直接流すようにするか?と思ったが\n\n[割り当てと上限  |  Cloud Logging  |  Google Cloud](https://cloud.google.com/logging/quotas?hl=ja)\n\n同じようにLoggingにも割り当て上限があるのでこの辺も考慮できていないといけない\n\nこの辺まで調べて面倒になってきてしまいこの手法は諦めた\n\nメモリリミットに達してしまったため回避方法はなさそう…APIのレスポンスをそのままWorkflows上でよしなにやるパターンは厳しいという結論になりました\n\nWorkflowsはあくまで各処理のオーケストレーションなのでWorkflows内にあまり処理を持ち込むべきではないっていう考え方なのかなと推測\n\n結局こういうパターンはFunctionsでGCSにデータ置く+BigQueryへloadってパターンがベターなのかな",
          "timeToRead": 2,
          "objectID": "18e04e5f-00f2-50a2-a8d7-b7ac41718457",
          "_snippetResult": {
            "text": {
              "value": "たいてそのレスポンスをそのまま__ais-highlight__BigQuer__/ais-highlight__yに突っ込むみたいなやつ\n\nプライ",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "July 13, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "Workflowsで Memory usage limit exeeded",
              "matchLevel": "none",
              "matchedWords": []
            },
            "slug": {
              "value": "/entries/workflows_logging___ais-highlight__bigquer__/ais-highlight__y_failed/",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "text": {
              "value": "\nはてなブログのAPIなど公開のAPIをたたいてそのレスポンスをそのまま__ais-highlight__BigQuer__/ais-highlight__yに突っ込むみたいなやつ\n\nプライベートなのと規模感が小さいのでちょっと冒険的な感じでやってみようと次のような構成で試みた\n\n- Workflowsではてなブックマークの公開APIをたたく\n- WorkflowsのログにAPIのレスポンスをそのまま流す\n- Logging→集約シンクで__ais-highlight__BigQuer__/ais-highlight__yにレコードを追加する\n\nFunctionsを新たに作らなくても良いしとりあえずの生データも保存できるしわりと省力で実現できるかと考えた\n\nある程度動作確認して問題なかったので自分のブログの全URLで実行したら次のようなエラーが出てしまった\n\n```shell\nExecution failed or cancelled.\nin step \"call_workflow_api\", routine \"call_workflow\", line: 88\nin step \"collect_hatena_bookmark_workflow\", routine \"main\", line: 35\n{\n  \"message\": \"Execution failed or cancelled.\",\n  \"operation\": {\n    \"argument\": \"{\\\"target_url\\\":\\\"https://swfz.hatenablog.com/entry/2018/12/22/080733\\\"}\",\n    \"endTime\": \"2021-07-10T12:01:03.749667278Z\",\n    \"error\": {\n      \"payload\": \"{\\\"message\\\":\\\"ResourceLimitError: Memory usage limit exceeded\\\",\\\"tags\\\":[\\\"ResourceLimitError\\\"]}\",\n      \"stackTrace\": {}\n    },\n    \"name\": \"projects/1111111111111/locations/us-central1/workflows/collect_hatena_bookmark_metrics/executions/c4a686eb-4d92-4e95-94f6-4257438131e0\",\n    \"startTime\": \"2021-07-10T12:01:02.693637032Z\",\n    \"state\": \"FAILED\",\n    \"workflowRevisionId\": \"000001-331\"\n  },\n  \"tags\": [\n    \"OperationError\"\n  ]\n}\n```\n\nバズって300前後のブックマークが付いたURLのレスポンスで発生した\n\n[割り当てと上限  |  ワークフロー  |  Google Cloud](https://cloud.google.com/workflows/quotas)\n\n変数のメモリ割り当てにも上限があり64KBまでらしい\n\nなのでAPIのレスポンスが64KB以上ある場合はエラーになってしまう…\n\nFunctionsは経由するがFunctionsからLoggingへ直接流すようにするか?と思ったが\n\n[割り当てと上限  |  Cloud Logging  |  Google Cloud](https://cloud.google.com/logging/quotas?hl=ja)\n\n同じようにLoggingにも割り当て上限があるのでこの辺も考慮できていないといけない\n\nこの辺まで調べて面倒になってきてしまいこの手法は諦めた\n\nメモリリミットに達してしまったため回避方法はなさそう…APIのレスポンスをそのままWorkflows上でよしなにやるパターンは厳しいという結論になりました\n\nWorkflowsはあくまで各処理のオーケストレーションなのでWorkflows内にあまり処理を持ち込むべきではないっていう考え方なのかなと推測\n\n結局こういうパターンはFunctionsでGCSにデータ置く+__ais-highlight__BigQuer__/ais-highlight__yへloadってパターンがベターなのかな",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "2",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        },
        {
          "date": "April 10, 2021",
          "title": "DataPortalで9時区切りの日付カラムを計算フィールドで作成する",
          "slug": "/entries/dataportal_experience_date/",
          "text": "\nただのメモ\n\nTogglからBigQueryにデータを突っ込んでいてそれをDataPortal経由でグラフ化している\n\n`start: 2020-12-25 21:52:30 UTC`（実際計測している時刻UTCではなくAsia/TokyoだがToggl側のAPIが返す値は時刻+UTCという値が返ってきている）\n\n上記のようなフォーマットのカラムを午前9時を堺にグルーピングしたいという要件が出てきた\n\n## 経緯\n\n単にグラフ化した場合`start`を基準にして日付単位でグループ化すると\n\n睡眠時間によっては日の合計時間が24時間を超えてしまうためグラフを眺めていて違和感がある\n\nそのため現在は0時をまたいで睡眠をとった場合は0時を境に分割して記録している\n\n```\n睡眠: 2021-04-09 22:00:00 ～ 2021-04-10 07:00:00\nToggl上での記録: \n- 2021-04-09 22:00:00 ～ 2021-04-10 00:00:00\n- 2021-04-10 00:00:00 ～ 2021-04-10 07:00:00\n```\n\nそうすると正確な日付としては分割して結果を閲覧できるが自分の体感としての日の睡眠時間がずれてグラフ化されてしまう\n\nそこで、冒頭のように午前9時開始を堺に日付を分割して0-9時のデータは前日分としてグラフ上では扱えるようにする\n\nそのための計算フィールドの計算式が下記\n\n```sql\nif(\n  parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT%H\",start)\n  ) < parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT09\",start)\n  ),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", datetime_sub(start, INTERVAL 1 DAY))),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", start))\n)\n```\n\n## つまずきポイント\n- DataPortalで日付カラムとして扱う場合はDate型かDateTime型になっている必要があるので結果に`parse_date`などパース処理が必要\n- `start`自体も日付・時刻カラムだったのでまず`format_datetime`でフォーマットしてからさらに`parse_datetime`で比較させて上げる必要があった\n- よく考えれば分かるはずだったがDataPortal上のテーブルで可視化すると別のフォーマットで出力されてしまい若干混乱した\n\nこんな感じで午前9時を境に日付を変更できた\n\n![alt](dataportal_experience_date01.png)",
          "timeToRead": 2,
          "objectID": "17cb94a3-6fe4-5894-bec2-e090afbbdb8d",
          "_snippetResult": {
            "text": {
              "value": "\nただのメモ\n\nTogglから__ais-highlight__BigQuer__/ais-highlight__yにデータを突っ込んでいてそれをDataPortal経由でグ",
              "matchLevel": "full"
            }
          },
          "_highlightResult": {
            "date": {
              "value": "April 10, 2021",
              "matchLevel": "none",
              "matchedWords": []
            },
            "title": {
              "value": "DataPortalで9時区切りの日付カラムを計算フィールドで作成する",
              "matchLevel": "none",
              "matchedWords": []
            },
            "slug": {
              "value": "/entries/dataportal_experience_date/",
              "matchLevel": "none",
              "matchedWords": []
            },
            "text": {
              "value": "\nただのメモ\n\nTogglから__ais-highlight__BigQuer__/ais-highlight__yにデータを突っ込んでいてそれをDataPortal経由でグラフ化している\n\n`start: 2020-12-25 21:52:30 UTC`（実際計測している時刻UTCではなくAsia/TokyoだがToggl側のAPIが返す値は時刻+UTCという値が返ってきている）\n\n上記のようなフォーマットのカラムを午前9時を堺にグルーピングしたいという要件が出てきた\n\n## 経緯\n\n単にグラフ化した場合`start`を基準にして日付単位でグループ化すると\n\n睡眠時間によっては日の合計時間が24時間を超えてしまうためグラフを眺めていて違和感がある\n\nそのため現在は0時をまたいで睡眠をとった場合は0時を境に分割して記録している\n\n```\n睡眠: 2021-04-09 22:00:00 ～ 2021-04-10 07:00:00\nToggl上での記録: \n- 2021-04-09 22:00:00 ～ 2021-04-10 00:00:00\n- 2021-04-10 00:00:00 ～ 2021-04-10 07:00:00\n```\n\nそうすると正確な日付としては分割して結果を閲覧できるが自分の体感としての日の睡眠時間がずれてグラフ化されてしまう\n\nそこで、冒頭のように午前9時開始を堺に日付を分割して0-9時のデータは前日分としてグラフ上では扱えるようにする\n\nそのための計算フィールドの計算式が下記\n\n```sql\nif(\n  parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT%H\",start)\n  ) < parse_datetime(\n    \"%Y-%m-%dT%H\",\n    format_datetime(\"%Y-%m-%dT09\",start)\n  ),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", datetime_sub(start, INTERVAL 1 DAY))),\n  parse_date(\"%Y-%m-%d\" ,format_datetime(\"%Y-%m-%d\", start))\n)\n```\n\n## つまずきポイント\n- DataPortalで日付カラムとして扱う場合はDate型かDateTime型になっている必要があるので結果に`parse_date`などパース処理が必要\n- `start`自体も日付・時刻カラムだったのでまず`format_datetime`でフォーマットしてからさらに`parse_datetime`で比較させて上げる必要があった\n- よく考えれば分かるはずだったがDataPortal上のテーブルで可視化すると別のフォーマットで出力されてしまい若干混乱した\n\nこんな感じで午前9時を境に日付を変更できた\n\n![alt](dataportal_experience_date01.png)",
              "matchLevel": "full",
              "fullyHighlighted": false,
              "matchedWords": ["bigquer"]
            },
            "timeToRead": {
              "value": "2",
              "matchLevel": "none",
              "matchedWords": []
            }
          }
        }
      ],
      "nbHits": 8,
      "page": 0,
      "nbPages": 1,
      "hitsPerPage": 20,
      "exhaustiveNbHits": true,
      "exhaustiveTypo": true,
      "query": "BigQuer",
      "params": "facets=%5B%5D&highlightPostTag=__%2Fais-highlight__&highlightPreTag=__ais-highlight__&query=BigQuer&tagFilters=",
      "index": "til",
      "renderingContent": {},
      "processingTimeMS": 3
    }
  ]
}
